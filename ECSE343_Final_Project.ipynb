{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YyO6h708WRmz"
      },
      "outputs": [],
      "source": [
        "# [TODO] Rename this file to [your student ID].py\n",
        "\n",
        "# DO NOT EDIT THESE IMPORT STATEMENTS!\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Ellipse\n",
        "from scipy.stats import multivariate_normal\n",
        "from time import sleep\n",
        "##########\n",
        "\n",
        "###########\n",
        "# DO NOT edit this function\n",
        "def EM(gmm):\n",
        "    \"\"\"\n",
        "    Runs the expectation-maximization algorithm on a GMM\n",
        "    \n",
        "    Input: \n",
        "    gmm (Class GMM): our GMM instance\n",
        "    \n",
        "    Returns: \n",
        "    Nothing, but it should modify gmm using the previously defined functions\n",
        "    \"\"\"\n",
        "    \n",
        "    #Log likelihood computation\n",
        "    if gmm.verbose:\n",
        "        print('Iteration: {:4d}'.format(0), flush = True)\n",
        "\n",
        "    # Compute mixture normalization for all the samples\n",
        "    normalization(gmm)\n",
        "\n",
        "    # Compute initial Log likelihoods\n",
        "    logLikelihood(gmm)\n",
        "          \n",
        "    # Repeat EM iterations\n",
        "    for n in range(1,gmm.max_iter):               \n",
        "        # Expectation step\n",
        "        expectation(gmm)\n",
        "\n",
        "        # Maximization step\n",
        "        maximization(gmm)\n",
        "        \n",
        "        # Update mixture normalization for all the samples\n",
        "        normalization(gmm)\n",
        "        \n",
        "        # Update the Log likelihood estimate\n",
        "        logLikelihood(gmm)\n",
        "\n",
        "        # Logging and plotting\n",
        "        if gmm.verbose:\n",
        "            print('Iteration: {:4d} - log likelihood: {:1.6f}'.format(n, gmm.log_likelihoods[-1]), flush = True)\n",
        "        \n",
        "        if gmm.do_plot:\n",
        "            #gmm.plotGMM(ellipse = True)\n",
        "            #plt.pause(0.05)\n",
        "            #sleep(0.15)\n",
        "            if n != gmm.max_iter - 1:\n",
        "                plt.close()\n",
        "            \n",
        "        # Compute the relative log-likelihood improvement and claim victory if a convergence tolerance is met\n",
        "        relative_error = abs(gmm.log_likelihoods[-2] / gmm.log_likelihoods[-1])\n",
        "        if (abs(1 - relative_error) < gmm.tol):\n",
        "            expectation(gmm)\n",
        "            if gmm.verbose:\n",
        "                print('SUCCESS: Your EM process converged.', flush = True)\n",
        "            return\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    if gmm.verbose:\n",
        "        print('ERROR: You ran out of iterations before converging.', flush = True)\n",
        "###########\n",
        "\n",
        "\n",
        "###########\n",
        "# DO NOT EDIT\n",
        "# Class that encapsulates the Gaussian mixture model data and utility methods\n",
        "class GMM:\n",
        "    def __init__( self, X, n_components = 10, reg_covar = 1e-2, tol = 1e-4, \n",
        "                  max_iter = 100, verbose = True, do_plot = False, mu_init = None):\n",
        "        \"\"\"\n",
        "        Constructor of the GMM class\n",
        "            \n",
        "        Inputs: \n",
        "        X (np.array((n_samples, n_dim))): array containing the n_samples n_dim-dimensional data samples\n",
        "        n_components (int): number of mixture components for our GMM\n",
        "        reg_covar (float): regularization value to add to the diagonal of the covariance matrices\n",
        "        tol (float): relative log-likelihood tolerance, at which point we will terminate the iterative EM algorithm\n",
        "        max_iter (int): maximum number of iterations, after which we terminate the iterative EM algorithm\n",
        "        mu_init (np.array((n_components, n_dim))): array containing initial means for each Gaussian in the mixture; if None, we will sample them from X.\n",
        "        verbose (bool): True to print verbose output\n",
        "        do_plot (bool): True to plot GMM evolution after each EM iteration\n",
        "        \"\"\"\n",
        "\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.n_samples, self.n_dim = self.X.shape\n",
        "        self.n_components = n_components\n",
        "        self.reg_covar = reg_covar**2\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "        self.verbose = verbose\n",
        "        self.do_plot = do_plot\n",
        "        self.reg_covar = reg_covar**2\n",
        "        \n",
        "        # regularization matrix\n",
        "        self.reg_cov = self.reg_covar * np.identity(self.n_dim, dtype = np.float32)\n",
        "        \n",
        "        # initial (isotropic) covariance extent\n",
        "        self.init_covar = 0.5 * (np.amax(X) - np.amin(X)) / self.n_components          \n",
        "        \n",
        "        # initial covariance matrix\n",
        "        self.init_cov = self.init_covar * np.identity(self.n_dim, dtype = np.float32) \n",
        "                \n",
        "        \n",
        "        # Initialize the mu, covariance and pi values\n",
        "        if mu_init is None:\n",
        "            # Initialize mean vector as random element of X\n",
        "            self.mu = self.X[np.random.choice(range(0,self.n_samples), self.n_components, replace=False),:]\n",
        "        else:\n",
        "            try:\n",
        "                assert( mu_init.shape[0] == self.n_components and mu_init.shape[1] == self.n_dim )\n",
        "            except:\n",
        "                raise Exception('Can\\'t plot if not 2D')\n",
        "            \n",
        "            # Initialize mean vector from the provided means mu_init\n",
        "            self.mu = mu_init \n",
        "        \n",
        "        # Initialize covariances as diagonal matrices (isotropic Gaussians)\n",
        "        self.cov = np.zeros((self.n_components, self.n_dim, self.n_dim), dtype=np.float32)\n",
        "        for c in range(self.n_components):\n",
        "            self.cov[c,:,:] = self.init_cov\n",
        "\n",
        "        # Python list of the n_components multivariate Gaussian distributions\n",
        "        # The .pdf method of the Gaussian's allows you to evaluate them at a vector of input locations\n",
        "        self.gauss = []\n",
        "        for c in range(self.n_components):\n",
        "            self.gauss.append( multivariate_normal( mean = self.mu[c,:], \n",
        "                                                    cov = self.cov[c,:,:]) )\n",
        "        \n",
        "        # Probabilities of selecting a specific Gaussian from the mixture\n",
        "        # Initialized to uniform probability for selecting each Gaussian, i.e., 1/K\n",
        "        self.pi = np.full(self.n_components, 1./self.n_components, dtype = np.float32)\n",
        "        \n",
        "        # The weight of each Gaussian in the mixture\n",
        "        # Initialized to 0\n",
        "        self.weight = np.zeros(self.n_components, dtype = np.float32)\n",
        "        \n",
        "        # The probabilities of sample X_i belonging to Gaussian N_c\n",
        "        # Initialized to 0\n",
        "        self.alpha = np.zeros((self.n_samples, self.n_components), dtype = np.float32)\n",
        "        \n",
        "        # Normalization for alpha\n",
        "        # Initialized to 0\n",
        "        self.beta = np.zeros(self.n_samples)\n",
        "        \n",
        "        # Latent labels (indices) of the Gaussian with maximum probability of having generated sample X_i\n",
        "        # Initialized to 0\n",
        "        self.Z = np.zeros(self.n_samples, dtype = np.int32)\n",
        "        \n",
        "        # Python list for logging the log-likelihood after each iteration of the EM algorithm\n",
        "        self.log_likelihoods = [] \n",
        "\n",
        "#############\n",
        "\n",
        "\n",
        "\n",
        "# [TODO] Deliverable 4: Computing the mixture normalization\n",
        "def normalization(gmm):     \n",
        "    \"\"\"\n",
        "    Compute the mixture normalization factor for all the data samples\n",
        "\n",
        "    Input: \n",
        "    gmm (Class GMM): our GMM instance\n",
        "\n",
        "    Returns: \n",
        "    Nothing, but you should modify gmm.beta\n",
        "    \"\"\"\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    n=gmm.n_samples\n",
        "    K=gmm.n_components\n",
        "    for i in range(n):\n",
        "      gmm.beta[i]=0\n",
        "      for c in range(K):\n",
        "        gmm.beta[i]=gmm.beta[i]+gmm.pi[c]*multivariate_normal.pdf(gmm.X[i],gmm.mu[c],gmm.cov[c])\n",
        "\n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "\n",
        "\n",
        "# [TODO] Deliverable 5: E-Step\n",
        "def expectation(gmm):           \n",
        "    \"\"\"\n",
        "    The expectation step\n",
        "\n",
        "    Input:\n",
        "    gmm (Class GMM): our GMM instance\n",
        "\n",
        "    Returns: \n",
        "    Nothing, but you should modify gmm.alpha\n",
        "    \"\"\"\n",
        "\n",
        "    m=gmm.n_samples\n",
        "    n=gmm.n_components\n",
        "    ### BEGIN SOLUTION\n",
        "    for i in range(m):\n",
        "      for j in range(n):\n",
        "        gmm.alpha[i,j]=gmm.pi[j]*multivariate_normal.pdf(gmm.X[i],gmm.mu[j],gmm.cov[j])/gmm.beta[i]\n",
        "    ### END SOLUTION\n",
        "\n",
        "\n",
        "\n",
        "# [TODO] Deliverable 6: M-Step\n",
        "def maximization(gmm):                   \n",
        "    \"\"\"\n",
        "    The maximization step\n",
        "    \n",
        "    Input: \n",
        "    gmm (Class GMM): our GMM instance\n",
        "    \n",
        "    Returns: \n",
        "    Nothing, but you should modify gmm.Z, gmm.weight, gmm.pi, gmm.mu, gmm.cov, and gmm.gauss    \n",
        "    \"\"\"\n",
        "    \n",
        "    # You can loop over the mixture components ONLY\n",
        "    # and assume that you already know alpha\n",
        "    # Hint 1: np.argmax is useful, here\n",
        "    # Hint 2: don't forgot to regularize your covariance matrices with gmm.reg_cov\n",
        "    \n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "    gmm.Z=np.argmax(gmm.alpha,1)\n",
        "    gmm.gauss=[]\n",
        "    for j in range(gmm.n_components):\n",
        "      gmm.weight[j]=np.sum(gmm.alpha[:,j])\n",
        "      gmm.pi[j]=gmm.weight[j]/gmm.n_samples\n",
        "\n",
        "    for j in range (gmm.n_components):\n",
        "      tmp=np.zeros(3)\n",
        "      for i in range(gmm.n_samples):\n",
        "        tmp=tmp+gmm.alpha[i,j]*gmm.X[i]\n",
        "      gmm.mu[j]=1/gmm.weight[j]*tmp\n",
        "      \n",
        "    for j in range(gmm.n_components):\n",
        "      sum=np.zeros((3,3))\n",
        "      for i in range(gmm.n_samples):\n",
        "        sum=sum+gmm.alpha[i,j]*np.dot(np.reshape((gmm.X[i]-gmm.mu[j]),(gmm.X[i].shape[0],1)),np.reshape((gmm.X[i]-gmm.mu[j]),(1,gmm.X[i].shape[0])))\n",
        "      gmm.cov[j]=1/gmm.weight[j]*sum+gmm.reg_cov\n",
        "      gmm.gauss.append(multivariate_normal(gmm.mu[j,:],gmm.cov[j,:,:]))\n",
        "    \n",
        "    \n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "\n",
        "# [TODO] Deliverable 7: Compute the log-likelihood\n",
        "def logLikelihood(gmm):                        \n",
        "    \"\"\"\n",
        "    Log-likelihood computation\n",
        "\n",
        "    Input: \n",
        "    gmm (Class GMM): our GMM instance\n",
        "\n",
        "    Returns: \n",
        "    Nothing, but you should modify gmm.log_likelihoods\n",
        "    \"\"\"\n",
        "\n",
        "    # Note: you need to append to gmm.log_likelihoods\n",
        "    \n",
        "    ### BEGIN SOLUTION\n",
        "    log_llh=0\n",
        "    for i in range(gmm.n_samples):\n",
        "      log_llh=log_llh+np.log(gmm.beta[i])\n",
        "    gmm.log_likelihoods.append(log_llh)\n",
        "    ### END SOLUTION\n",
        "\n",
        "\n",
        "\n",
        "# Some example test routines for the deliverables. \n",
        "# Feel free to write and include your own tests here.\n",
        "# Code in this main block will not count for credit, \n",
        "# but the collaboration and plagiarism policies still hold.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtRquApdgfx_",
        "outputId": "497504c4-2195-422c-98f9-5fb16b3a95a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:    0\n",
            "Iteration:    1 - log likelihood: -9199.391168\n",
            "Iteration:    2 - log likelihood: -9110.509489\n",
            "Iteration:    3 - log likelihood: -9088.221866\n",
            "Iteration:    4 - log likelihood: -9084.196603\n",
            "Iteration:    5 - log likelihood: -9081.447321\n",
            "Iteration:    6 - log likelihood: -9079.245498\n",
            "Iteration:    7 - log likelihood: -9077.348893\n",
            "Iteration:    8 - log likelihood: -9075.656557\n",
            "Iteration:    9 - log likelihood: -9074.130613\n",
            "Iteration:   10 - log likelihood: -9072.764302\n",
            "Iteration:   11 - log likelihood: -9071.550772\n",
            "Iteration:   12 - log likelihood: -9070.455282\n",
            "Iteration:   13 - log likelihood: -9069.406436\n",
            "Iteration:   14 - log likelihood: -9068.305793\n",
            "Iteration:   15 - log likelihood: -9067.021943\n",
            "Iteration:   16 - log likelihood: -9065.300669\n",
            "Iteration:   17 - log likelihood: -9062.539302\n",
            "Iteration:   18 - log likelihood: -9057.787227\n",
            "Iteration:   19 - log likelihood: -9053.411736\n",
            "Iteration:   20 - log likelihood: -9051.078176\n",
            "Iteration:   21 - log likelihood: -9049.782566\n",
            "Iteration:   22 - log likelihood: -9049.002302\n",
            "Iteration:   23 - log likelihood: -9048.413814\n",
            "Iteration:   24 - log likelihood: -9047.893904\n",
            "Iteration:   25 - log likelihood: -9047.388944\n",
            "Iteration:   26 - log likelihood: -9046.863636\n",
            "Iteration:   27 - log likelihood: -9046.284102\n",
            "Iteration:   28 - log likelihood: -9045.614168\n",
            "Iteration:   29 - log likelihood: -9044.828041\n",
            "Iteration:   30 - log likelihood: -9043.920775\n",
            "Iteration:   31 - log likelihood: -9042.883191\n",
            "Iteration:   32 - log likelihood: -9041.678725\n",
            "Iteration:   33 - log likelihood: -9040.248914\n",
            "Iteration:   34 - log likelihood: -9038.517154\n",
            "Iteration:   35 - log likelihood: -9036.389684\n",
            "Iteration:   36 - log likelihood: -9033.761164\n",
            "Iteration:   37 - log likelihood: -9030.535360\n",
            "Iteration:   38 - log likelihood: -9026.689007\n",
            "Iteration:   39 - log likelihood: -9022.378699\n",
            "Iteration:   40 - log likelihood: -9017.958619\n",
            "Iteration:   41 - log likelihood: -9013.790494\n",
            "Iteration:   42 - log likelihood: -9010.068996\n",
            "Iteration:   43 - log likelihood: -9006.841122\n",
            "Iteration:   44 - log likelihood: -9004.072115\n",
            "Iteration:   45 - log likelihood: -9001.678743\n",
            "Iteration:   46 - log likelihood: -8999.564885\n",
            "Iteration:   47 - log likelihood: -8997.645724\n",
            "Iteration:   48 - log likelihood: -8995.851844\n",
            "Iteration:   49 - log likelihood: -8994.127696\n",
            "Iteration:   50 - log likelihood: -8992.435017\n",
            "Iteration:   51 - log likelihood: -8990.761426\n",
            "Iteration:   52 - log likelihood: -8989.128648\n",
            "Iteration:   53 - log likelihood: -8987.589715\n",
            "Iteration:   54 - log likelihood: -8986.208166\n",
            "Iteration:   55 - log likelihood: -8985.025549\n",
            "Iteration:   56 - log likelihood: -8984.037835\n",
            "Iteration:   57 - log likelihood: -8983.191023\n",
            "Iteration:   58 - log likelihood: -8982.397843\n",
            "Iteration:   59 - log likelihood: -8981.565185\n",
            "Iteration:   60 - log likelihood: -8980.628671\n",
            "Iteration:   61 - log likelihood: -8979.575509\n",
            "Iteration:   62 - log likelihood: -8978.390360\n",
            "Iteration:   63 - log likelihood: -8976.955892\n",
            "Iteration:   64 - log likelihood: -8974.868327\n",
            "Iteration:   65 - log likelihood: -8970.462449\n",
            "Iteration:   66 - log likelihood: -8963.810266\n",
            "Iteration:   67 - log likelihood: -8958.841221\n",
            "Iteration:   68 - log likelihood: -8953.990977\n",
            "Iteration:   69 - log likelihood: -8947.903172\n",
            "Iteration:   70 - log likelihood: -8939.365746\n",
            "Iteration:   71 - log likelihood: -8929.862155\n",
            "Iteration:   72 - log likelihood: -8921.284230\n",
            "Iteration:   73 - log likelihood: -8912.293258\n",
            "Iteration:   74 - log likelihood: -8901.932893\n",
            "Iteration:   75 - log likelihood: -8886.522931\n",
            "Iteration:   76 - log likelihood: -8878.467613\n",
            "Iteration:   77 - log likelihood: -8876.850563\n",
            "Iteration:   78 - log likelihood: -8876.305916\n",
            "Iteration:   79 - log likelihood: -8876.034757\n",
            "Iteration:   80 - log likelihood: -8875.885417\n",
            "Iteration:   81 - log likelihood: -8875.799067\n",
            "Iteration:   82 - log likelihood: -8875.746283\n",
            "Iteration:   83 - log likelihood: -8875.711542\n",
            "Iteration:   84 - log likelihood: -8875.686443\n",
            "Iteration:   85 - log likelihood: -8875.666461\n",
            "Iteration:   86 - log likelihood: -8875.649004\n",
            "Iteration:   87 - log likelihood: -8875.632679\n",
            "Iteration:   88 - log likelihood: -8875.616655\n",
            "Iteration:   89 - log likelihood: -8875.600484\n",
            "Iteration:   90 - log likelihood: -8875.583833\n",
            "Iteration:   91 - log likelihood: -8875.566515\n",
            "Iteration:   92 - log likelihood: -8875.548285\n",
            "Iteration:   93 - log likelihood: -8875.529132\n",
            "Iteration:   94 - log likelihood: -8875.508904\n",
            "Iteration:   95 - log likelihood: -8875.487609\n",
            "Iteration:   96 - log likelihood: -8875.465210\n",
            "Iteration:   97 - log likelihood: -8875.441778\n",
            "Iteration:   98 - log likelihood: -8875.417315\n",
            "Iteration:   99 - log likelihood: -8875.392018\n",
            "ERROR: You ran out of iterations before converging.\n",
            "Iteration:    0\n",
            "Iteration:    1 - log likelihood: -36120.157395\n",
            "Iteration:    2 - log likelihood: -36095.423953\n",
            "Iteration:    3 - log likelihood: -36085.167722\n",
            "Iteration:    4 - log likelihood: -36079.868432\n",
            "Iteration:    5 - log likelihood: -36076.935678\n",
            "Iteration:    6 - log likelihood: -36075.416676\n",
            "Iteration:    7 - log likelihood: -36074.639816\n",
            "Iteration:    8 - log likelihood: -36074.214522\n",
            "Iteration:    9 - log likelihood: -36073.965884\n",
            "Iteration:   10 - log likelihood: -36073.814361\n",
            "Iteration:   11 - log likelihood: -36073.719960\n",
            "Iteration:   12 - log likelihood: -36073.660474\n",
            "Iteration:   13 - log likelihood: -36073.622407\n",
            "Iteration:   14 - log likelihood: -36073.597839\n",
            "SUCCESS: Your EM process converged.\n",
            "Iteration:    0\n",
            "Iteration:    1 - log likelihood: -147438.168880\n",
            "Iteration:    2 - log likelihood: -147410.693980\n",
            "Iteration:    3 - log likelihood: -147403.161302\n",
            "Iteration:    4 - log likelihood: -147399.754031\n",
            "Iteration:    5 - log likelihood: -147397.739716\n",
            "Iteration:    6 - log likelihood: -147396.379259\n",
            "Iteration:    7 - log likelihood: -147395.393299\n",
            "Iteration:    8 - log likelihood: -147394.653684\n",
            "Iteration:    9 - log likelihood: -147394.087885\n",
            "Iteration:   10 - log likelihood: -147393.649630\n",
            "Iteration:   11 - log likelihood: -147393.308045\n",
            "Iteration:   12 - log likelihood: -147393.042336\n",
            "Iteration:   13 - log likelihood: -147392.834463\n",
            "Iteration:   14 - log likelihood: -147392.672427\n",
            "Iteration:   15 - log likelihood: -147392.547503\n",
            "SUCCESS: Your EM process converged.\n",
            "Iteration:    0\n",
            "Iteration:    1 - log likelihood: -594920.152442\n",
            "Iteration:    2 - log likelihood: -594892.015121\n",
            "Iteration:    3 - log likelihood: -594878.673752\n",
            "Iteration:    4 - log likelihood: -594869.800862\n",
            "Iteration:    5 - log likelihood: -594863.330008\n",
            "Iteration:    6 - log likelihood: -594858.494506\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import cv2\n",
        "######################\n",
        "#Read the Image in\n",
        "home=cv2.imread('home.jpg')\n",
        "#Adjust the RGB value for the display\n",
        "home=cv2.cvtColor(home, cv2.COLOR_BGR2RGB)\n",
        "#Adjust the size of the input image \n",
        "#home=cv2.resize(home,(128,96),interpolation = cv2.INTER_AREA)\n",
        "height, width, channel=home.shape\n",
        "\n",
        "#Set the number of segment color \n",
        "n_components=5\n",
        "#Set the numbers of iterations from downscale/upscale for speed up\n",
        "iterations=5\n",
        "\n",
        "#Initialize the temporary variables for the iteration\n",
        "tmpZ=np.zeros((int (width/(2**6)),int (height/(2**6))))\n",
        "tmpmu=np.zeros((n_components,channel))\n",
        "tmpcov=np.zeros((n_components, channel, channel), dtype=np.float32)\n",
        "tmppi=np.full(n_components, 1./n_components, dtype = np.float32)\n",
        "\n",
        "#Downscale the resolution of the image and process to obtain the approximate GMM model within \n",
        "#short period of time, use the obtained GMM model and  upscale the image so that the model processing\n",
        "#doesn't start from the pure beginning. Iterate the same until the image resolution is back to the original resolution\n",
        "\n",
        "for numiter in range(iterations):\n",
        "  #Set tmphome as the resized version of home corresponding to which iteration the loop gets to\n",
        "  tmphome=cv2.resize(home,(int(width/(2**(iterations-numiter-1))),int(height/(2**(iterations-numiter-1)))),interpolation = cv2.INTER_AREA)\n",
        "  tmph, tmpw, tmpc= tmphome.shape\n",
        "  tmphome=tmphome.reshape(tmph*tmpw,tmpc)\n",
        "  maxiter=100\n",
        "  if numiter>(iterations-3) and numiter<(iterations-2):\n",
        "    maxiter=70\n",
        "  elif numiter==(iterations-2):\n",
        "    maxiter=20\n",
        "  elif numiter==(iterations-1):\n",
        "    maxiter=10\n",
        "  #Initialize the GMM model\n",
        "  gmm_test=GMM( tmphome, n_components, reg_covar = 1e-3, \n",
        "                    tol = 1e-6, max_iter = maxiter, \n",
        "                    verbose = True, do_plot = True) \n",
        "  #Replace the initial value of Z, mu, cov and pi with the values obtained from the last iteration\n",
        "  if(numiter!=0):\n",
        "    newZ=np.zeros((tmpw,tmph), dtype = np.int32)\n",
        "    for i in range(tmpw):\n",
        "      for j in range(tmph):\n",
        "        newZ[i,j]= tmpZ[int (i/2),int (j/2)]\n",
        "    newZ=newZ.reshape(tmpw*tmph)\n",
        "    gmm_test.Z=newZ\n",
        "    gmm_test.mu=tmpmu\n",
        "    gmm_test.cov=tmpcov\n",
        "    gmm_test.pi=tmppi\n",
        "\n",
        "  EM(gmm_test)  \n",
        "  # Record the Z, mu, cov, pi values for the next iteration\n",
        "  tmpZ=gmm_test.Z.reshape(tmpw,tmph)\n",
        "  tmpmu=gmm_test.mu\n",
        "  tmpcov=gmm_test.cov\n",
        "  tmppi=gmm_test.pi\n",
        "  \n",
        "home = home.reshape(height*width,channel)\n",
        "#Replace the pixel values of the original image with the mean values\n",
        "for i in range(height*width):\n",
        "  home[i,:]=gmm_test.mu[gmm_test.Z[i]]\n",
        "\n",
        "home=home.reshape(height,width,channel)\n",
        "plt.imshow(home)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4QnyypDpwmW",
        "outputId": "bbb86edc-ea6c-49f6-a57f-6b59a208e353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cv2.imwrite('resultimg.jpg',cv2.cvtColor(home, cv2.COLOR_RGB2BGR))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ECSE343 Final Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}