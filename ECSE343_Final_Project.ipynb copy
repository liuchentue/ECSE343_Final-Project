{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YyO6h708WRmz"
      },
      "outputs": [],
      "source": [
        "# [TODO] Rename this file to [your student ID].py\n",
        "\n",
        "# DO NOT EDIT THESE IMPORT STATEMENTS!\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Ellipse\n",
        "from scipy.stats import multivariate_normal\n",
        "from time import sleep\n",
        "##########\n",
        "\n",
        "###########\n",
        "# DO NOT edit this function\n",
        "def EM(gmm):\n",
        "    \"\"\"\n",
        "    Runs the expectation-maximization algorithm on a GMM\n",
        "    \n",
        "    Input: \n",
        "    gmm (Class GMM): our GMM instance\n",
        "    \n",
        "    Returns: \n",
        "    Nothing, but it should modify gmm using the previously defined functions\n",
        "    \"\"\"\n",
        "    \n",
        "    #Log likelihood computation\n",
        "    if gmm.verbose:\n",
        "        print('Iteration: {:4d}'.format(0), flush = True)\n",
        "\n",
        "    # Compute mixture normalization for all the samples\n",
        "    normalization(gmm)\n",
        "\n",
        "    # Compute initial Log likelihoods\n",
        "    logLikelihood(gmm)\n",
        "          \n",
        "    # Repeat EM iterations\n",
        "    for n in range(1,gmm.max_iter):               \n",
        "        # Expectation step\n",
        "        expectation(gmm)\n",
        "\n",
        "        # Maximization step\n",
        "        maximization(gmm)\n",
        "        \n",
        "        # Update mixture normalization for all the samples\n",
        "        normalization(gmm)\n",
        "        \n",
        "        # Update the Log likelihood estimate\n",
        "        logLikelihood(gmm)\n",
        "\n",
        "        # Logging and plotting\n",
        "        if gmm.verbose:\n",
        "            print('Iteration: {:4d} - log likelihood: {:1.6f}'.format(n, gmm.log_likelihoods[-1]), flush = True)\n",
        "        \n",
        "        if gmm.do_plot:\n",
        "            #gmm.plotGMM(ellipse = True)\n",
        "            #plt.pause(0.05)\n",
        "            #sleep(0.15)\n",
        "            if n != gmm.max_iter - 1:\n",
        "                plt.close()\n",
        "            \n",
        "        # Compute the relative log-likelihood improvement and claim victory if a convergence tolerance is met\n",
        "        relative_error = abs(gmm.log_likelihoods[-2] / gmm.log_likelihoods[-1])\n",
        "        if (abs(1 - relative_error) < gmm.tol):\n",
        "            expectation(gmm)\n",
        "            if gmm.verbose:\n",
        "                print('SUCCESS: Your EM process converged.', flush = True)\n",
        "            return\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    if gmm.verbose:\n",
        "        print('ERROR: You ran out of iterations before converging.', flush = True)\n",
        "###########\n",
        "\n",
        "\n",
        "###########\n",
        "# DO NOT EDIT\n",
        "# Class that encapsulates the Gaussian mixture model data and utility methods\n",
        "class GMM:\n",
        "    def __init__( self, X, n_components = 10, reg_covar = 1e-2, tol = 1e-4, \n",
        "                  max_iter = 100, verbose = True, do_plot = False, mu_init = None):\n",
        "        \"\"\"\n",
        "        Constructor of the GMM class\n",
        "            \n",
        "        Inputs: \n",
        "        X (np.array((n_samples, n_dim))): array containing the n_samples n_dim-dimensional data samples\n",
        "        n_components (int): number of mixture components for our GMM\n",
        "        reg_covar (float): regularization value to add to the diagonal of the covariance matrices\n",
        "        tol (float): relative log-likelihood tolerance, at which point we will terminate the iterative EM algorithm\n",
        "        max_iter (int): maximum number of iterations, after which we terminate the iterative EM algorithm\n",
        "        mu_init (np.array((n_components, n_dim))): array containing initial means for each Gaussian in the mixture; if None, we will sample them from X.\n",
        "        verbose (bool): True to print verbose output\n",
        "        do_plot (bool): True to plot GMM evolution after each EM iteration\n",
        "        \"\"\"\n",
        "\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.n_samples, self.n_dim = self.X.shape\n",
        "        self.n_components = n_components\n",
        "        self.reg_covar = reg_covar**2\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "        self.verbose = verbose\n",
        "        self.do_plot = do_plot\n",
        "        self.reg_covar = reg_covar**2\n",
        "        \n",
        "        # regularization matrix\n",
        "        self.reg_cov = self.reg_covar * np.identity(self.n_dim, dtype = np.float32)\n",
        "        \n",
        "        # initial (isotropic) covariance extent\n",
        "        self.init_covar = 0.5 * (np.amax(X) - np.amin(X)) / self.n_components          \n",
        "        \n",
        "        # initial covariance matrix\n",
        "        self.init_cov = self.init_covar * np.identity(self.n_dim, dtype = np.float32) \n",
        "                \n",
        "        \n",
        "        # Initialize the mu, covariance and pi values\n",
        "        if mu_init is None:\n",
        "            # Initialize mean vector as random element of X\n",
        "            self.mu = self.X[np.random.choice(range(0,self.n_samples), self.n_components, replace=False),:]\n",
        "        else:\n",
        "            try:\n",
        "                assert( mu_init.shape[0] == self.n_components and mu_init.shape[1] == self.n_dim )\n",
        "            except:\n",
        "                raise Exception('Can\\'t plot if not 2D')\n",
        "            \n",
        "            # Initialize mean vector from the provided means mu_init\n",
        "            self.mu = mu_init \n",
        "        \n",
        "        # Initialize covariances as diagonal matrices (isotropic Gaussians)\n",
        "        self.cov = np.zeros((self.n_components, self.n_dim, self.n_dim), dtype=np.float32)\n",
        "        for c in range(self.n_components):\n",
        "            self.cov[c,:,:] = self.init_cov\n",
        "\n",
        "        # Python list of the n_components multivariate Gaussian distributions\n",
        "        # The .pdf method of the Gaussian's allows you to evaluate them at a vector of input locations\n",
        "        self.gauss = []\n",
        "        for c in range(self.n_components):\n",
        "            self.gauss.append( multivariate_normal( mean = self.mu[c,:], \n",
        "                                                    cov = self.cov[c,:,:]) )\n",
        "        \n",
        "        # Probabilities of selecting a specific Gaussian from the mixture\n",
        "        # Initialized to uniform probability for selecting each Gaussian, i.e., 1/K\n",
        "        self.pi = np.full(self.n_components, 1./self.n_components, dtype = np.float32)\n",
        "        \n",
        "        # The weight of each Gaussian in the mixture\n",
        "        # Initialized to 0\n",
        "        self.weight = np.zeros(self.n_components, dtype = np.float32)\n",
        "        \n",
        "        # The probabilities of sample X_i belonging to Gaussian N_c\n",
        "        # Initialized to 0\n",
        "        self.alpha = np.zeros((self.n_samples, self.n_components), dtype = np.float32)\n",
        "        \n",
        "        # Normalization for alpha\n",
        "        # Initialized to 0\n",
        "        self.beta = np.zeros(self.n_samples)\n",
        "        \n",
        "        # Latent labels (indices) of the Gaussian with maximum probability of having generated sample X_i\n",
        "        # Initialized to 0\n",
        "        self.Z = np.zeros(self.n_samples, dtype = np.int32)\n",
        "        \n",
        "        # Python list for logging the log-likelihood after each iteration of the EM algorithm\n",
        "        self.log_likelihoods = [] \n",
        "\n",
        "#############\n",
        "\n",
        "\n",
        "\n",
        "# [TODO] Deliverable 4: Computing the mixture normalization\n",
        "def normalization(gmm):     \n",
        "    \"\"\"\n",
        "    Compute the mixture normalization factor for all the data samples\n",
        "\n",
        "    Input: \n",
        "    gmm (Class GMM): our GMM instance\n",
        "\n",
        "    Returns: \n",
        "    Nothing, but you should modify gmm.beta\n",
        "    \"\"\"\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    n=gmm.n_samples\n",
        "    K=gmm.n_components\n",
        "    for i in range(n):\n",
        "      gmm.beta[i]=0\n",
        "      for c in range(K):\n",
        "        gmm.beta[i]=gmm.beta[i]+gmm.pi[c]*multivariate_normal.pdf(gmm.X[i],gmm.mu[c],gmm.cov[c])\n",
        "\n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "\n",
        "\n",
        "# [TODO] Deliverable 5: E-Step\n",
        "def expectation(gmm):           \n",
        "    \"\"\"\n",
        "    The expectation step\n",
        "\n",
        "    Input:\n",
        "    gmm (Class GMM): our GMM instance\n",
        "\n",
        "    Returns: \n",
        "    Nothing, but you should modify gmm.alpha\n",
        "    \"\"\"\n",
        "\n",
        "    m=gmm.n_samples\n",
        "    n=gmm.n_components\n",
        "    ### BEGIN SOLUTION\n",
        "    for i in range(m):\n",
        "      for j in range(n):\n",
        "        gmm.alpha[i,j]=gmm.pi[j]*multivariate_normal.pdf(gmm.X[i],gmm.mu[j],gmm.cov[j])/gmm.beta[i]\n",
        "    ### END SOLUTION\n",
        "\n",
        "\n",
        "\n",
        "# [TODO] Deliverable 6: M-Step\n",
        "def maximization(gmm):                   \n",
        "    \"\"\"\n",
        "    The maximization step\n",
        "    \n",
        "    Input: \n",
        "    gmm (Class GMM): our GMM instance\n",
        "    \n",
        "    Returns: \n",
        "    Nothing, but you should modify gmm.Z, gmm.weight, gmm.pi, gmm.mu, gmm.cov, and gmm.gauss    \n",
        "    \"\"\"\n",
        "    \n",
        "    # You can loop over the mixture components ONLY\n",
        "    # and assume that you already know alpha\n",
        "    # Hint 1: np.argmax is useful, here\n",
        "    # Hint 2: don't forgot to regularize your covariance matrices with gmm.reg_cov\n",
        "    \n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "    gmm.Z=np.argmax(gmm.alpha,1)\n",
        "    gmm.gauss=[]\n",
        "    for j in range(gmm.n_components):\n",
        "      gmm.weight[j]=np.sum(gmm.alpha[:,j])\n",
        "      gmm.pi[j]=gmm.weight[j]/gmm.n_samples\n",
        "\n",
        "    for j in range (gmm.n_components):\n",
        "      tmp=np.zeros(3)\n",
        "      for i in range(gmm.n_samples):\n",
        "        tmp=tmp+gmm.alpha[i,j]*gmm.X[i]\n",
        "      gmm.mu[j]=1/gmm.weight[j]*tmp\n",
        "      \n",
        "    for j in range(gmm.n_components):\n",
        "      sum=np.zeros((3,3))\n",
        "      for i in range(gmm.n_samples):\n",
        "        sum=sum+gmm.alpha[i,j]*np.dot(np.reshape((gmm.X[i]-gmm.mu[j]),(gmm.X[i].shape[0],1)),np.reshape((gmm.X[i]-gmm.mu[j]),(1,gmm.X[i].shape[0])))\n",
        "      gmm.cov[j]=1/gmm.weight[j]*sum+gmm.reg_cov\n",
        "      gmm.gauss.append(multivariate_normal(gmm.mu[j,:],gmm.cov[j,:,:]))\n",
        "    \n",
        "    \n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "\n",
        "# [TODO] Deliverable 7: Compute the log-likelihood\n",
        "def logLikelihood(gmm):                        \n",
        "    \"\"\"\n",
        "    Log-likelihood computation\n",
        "\n",
        "    Input: \n",
        "    gmm (Class GMM): our GMM instance\n",
        "\n",
        "    Returns: \n",
        "    Nothing, but you should modify gmm.log_likelihoods\n",
        "    \"\"\"\n",
        "\n",
        "    # Note: you need to append to gmm.log_likelihoods\n",
        "    \n",
        "    ### BEGIN SOLUTION\n",
        "    log_llh=0\n",
        "    for i in range(gmm.n_samples):\n",
        "      log_llh=log_llh+np.log(gmm.beta[i])\n",
        "    gmm.log_likelihoods.append(log_llh)\n",
        "    ### END SOLUTION\n",
        "\n",
        "\n",
        "\n",
        "# Some example test routines for the deliverables. \n",
        "# Feel free to write and include your own tests here.\n",
        "# Code in this main block will not count for credit, \n",
        "# but the collaboration and plagiarism policies still hold.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtRquApdgfx_",
        "outputId": "1cdb93bd-dd8e-46f0-9e08-fca985be9552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:    0\n",
            "Iteration:    1 - log likelihood: -551.557114\n",
            "Iteration:    2 - log likelihood: -549.141065\n",
            "Iteration:    3 - log likelihood: -543.102671\n",
            "Iteration:    4 - log likelihood: -539.482155\n",
            "Iteration:    5 - log likelihood: -539.479358\n",
            "Iteration:    6 - log likelihood: -539.479147\n",
            "SUCCESS: Your EM process converged.\n",
            "Iteration:    0\n",
            "Iteration:    1 - log likelihood: -2271.764758\n",
            "Iteration:    2 - log likelihood: -2263.638168\n",
            "Iteration:    3 - log likelihood: -2260.183300\n",
            "Iteration:    4 - log likelihood: -2256.363925\n",
            "Iteration:    5 - log likelihood: -2249.683421\n",
            "Iteration:    6 - log likelihood: -2236.993254\n",
            "Iteration:    7 - log likelihood: -2222.063451\n",
            "Iteration:    8 - log likelihood: -2217.391882\n",
            "Iteration:    9 - log likelihood: -2215.686444\n",
            "Iteration:   10 - log likelihood: -2213.630887\n",
            "Iteration:   11 - log likelihood: -2207.462080\n",
            "Iteration:   12 - log likelihood: -2203.769375\n",
            "Iteration:   13 - log likelihood: -2203.169990\n",
            "Iteration:   14 - log likelihood: -2202.822268\n",
            "Iteration:   15 - log likelihood: -2202.578867\n",
            "Iteration:   16 - log likelihood: -2202.396965\n",
            "Iteration:   17 - log likelihood: -2202.254629\n",
            "Iteration:   18 - log likelihood: -2202.138393\n",
            "Iteration:   19 - log likelihood: -2202.040329\n",
            "Iteration:   20 - log likelihood: -2201.956058\n",
            "Iteration:   21 - log likelihood: -2201.883298\n",
            "Iteration:   22 - log likelihood: -2201.820596\n",
            "Iteration:   23 - log likelihood: -2201.766864\n",
            "Iteration:   24 - log likelihood: -2201.721049\n",
            "Iteration:   25 - log likelihood: -2201.682111\n",
            "Iteration:   26 - log likelihood: -2201.649104\n",
            "Iteration:   27 - log likelihood: -2201.621166\n",
            "Iteration:   28 - log likelihood: -2201.597566\n",
            "Iteration:   29 - log likelihood: -2201.577694\n",
            "Iteration:   30 - log likelihood: -2201.561041\n",
            "Iteration:   31 - log likelihood: -2201.547163\n",
            "Iteration:   32 - log likelihood: -2201.535703\n",
            "Iteration:   33 - log likelihood: -2201.526311\n",
            "Iteration:   34 - log likelihood: -2201.518684\n",
            "Iteration:   35 - log likelihood: -2201.512544\n",
            "Iteration:   36 - log likelihood: -2201.507646\n",
            "Iteration:   37 - log likelihood: -2201.503787\n",
            "Iteration:   38 - log likelihood: -2201.500764\n",
            "Iteration:   39 - log likelihood: -2201.498407\n",
            "Iteration:   40 - log likelihood: -2201.496597\n",
            "SUCCESS: Your EM process converged.\n",
            "Iteration:    0\n",
            "Iteration:    1 - log likelihood: -9075.242427\n",
            "Iteration:    2 - log likelihood: -9052.825613\n",
            "Iteration:    3 - log likelihood: -9035.588408\n",
            "Iteration:    4 - log likelihood: -9018.256075\n",
            "Iteration:    5 - log likelihood: -9003.848105\n",
            "Iteration:    6 - log likelihood: -8993.045216\n",
            "Iteration:    7 - log likelihood: -8983.066121\n",
            "Iteration:    8 - log likelihood: -8974.567792\n",
            "Iteration:    9 - log likelihood: -8970.008600\n",
            "Iteration:   10 - log likelihood: -8967.502490\n",
            "Iteration:   11 - log likelihood: -8965.017476\n",
            "Iteration:   12 - log likelihood: -8962.624315\n",
            "Iteration:   13 - log likelihood: -8960.824781\n",
            "Iteration:   14 - log likelihood: -8959.746068\n",
            "Iteration:   15 - log likelihood: -8959.186942\n",
            "Iteration:   16 - log likelihood: -8958.916399\n",
            "Iteration:   17 - log likelihood: -8958.788824\n",
            "Iteration:   18 - log likelihood: -8958.729138\n",
            "Iteration:   19 - log likelihood: -8958.701329\n",
            "Iteration:   20 - log likelihood: -8958.688369\n",
            "Iteration:   21 - log likelihood: -8958.682362\n",
            "SUCCESS: Your EM process converged.\n",
            "Iteration:    0\n",
            "Iteration:    1 - log likelihood: -36456.339283\n",
            "Iteration:    2 - log likelihood: -36451.133536\n",
            "Iteration:    3 - log likelihood: -36449.584240\n",
            "Iteration:    4 - log likelihood: -36448.853913\n",
            "Iteration:    5 - log likelihood: -36448.462063\n",
            "Iteration:    6 - log likelihood: -36448.244426\n",
            "Iteration:    7 - log likelihood: -36448.122672\n",
            "Iteration:    8 - log likelihood: -36448.054504\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import cv2\n",
        "######################\n",
        "#Read the Image in\n",
        "home=cv2.imread('home.jpg')\n",
        "#Adjust the RGB value for the display\n",
        "home=cv2.cvtColor(home, cv2.COLOR_BGR2RGB)\n",
        "#Adjust the size of the input image \n",
        "#home=cv2.resize(home,(256,192),interpolation = cv2.INTER_AREA)\n",
        "height, width, channel=home.shape\n",
        "\n",
        "#Set the number of segment color \n",
        "n_components=4\n",
        "\n",
        "#Initialize the temporary variables for the iteration\n",
        "tmpZ=np.zeros((int (width/(2**6)),int (height/(2**6))))\n",
        "tmpmu=np.zeros((n_components,channel))\n",
        "tmpcov=np.zeros((n_components, channel, channel), dtype=np.float32)\n",
        "tmppi=np.full(n_components, 1./n_components, dtype = np.float32)\n",
        "\n",
        "#Downscale the resolution of the image and process to obtain the approximate GMM model within \n",
        "#short period of time, use the obtained GMM model and  upscale the image so that the model processing\n",
        "#doesn't start from the pure beginning. Iterate the same until the image resolution is back to the original resolution\n",
        "\n",
        "for numiter in range(7):\n",
        "  #Set tmphome as the resized version of home corresponding to which iteration the loop gets to\n",
        "  tmphome=cv2.resize(home,(int(width/(2**(6-numiter))),int(height/(2**(6-numiter)))),interpolation = cv2.INTER_AREA)\n",
        "  tmph, tmpw, tmpc= tmphome.shape\n",
        "  tmphome=tmphome.reshape(tmph*tmpw,tmpc)\n",
        "  maxiter=100\n",
        "  if numiter>3 and numiter<5:\n",
        "    maxiter=70\n",
        "  elif numiter==5:\n",
        "    maxiter=20\n",
        "  elif numiter==6:\n",
        "    maxiter=10\n",
        "  #Initialize the GMM model\n",
        "  gmm_test=GMM( tmphome, n_components, reg_covar = 1e-3, \n",
        "                    tol = 1e-6, max_iter = maxiter, \n",
        "                    verbose = True, do_plot = True) \n",
        "  #Replace the initial value of Z, mu, cov and pi with the values obtained from the last iteration\n",
        "  if(numiter!=0):\n",
        "    newZ=np.zeros((tmpw,tmph), dtype = np.int32)\n",
        "    for i in range(tmpw):\n",
        "      for j in range(tmph):\n",
        "        newZ[i,j]= tmpZ[int (i/2),int (j/2)]\n",
        "    newZ=newZ.reshape(tmpw*tmph)\n",
        "    gmm_test.Z=newZ\n",
        "    gmm_test.mu=tmpmu\n",
        "    gmm_test.cov=tmpcov\n",
        "    gmm_test.pi=tmppi\n",
        "\n",
        "  EM(gmm_test)  \n",
        "  # Record the Z, mu, cov, pi values for the next iteration\n",
        "  tmpZ=gmm_test.Z.reshape(tmpw,tmph)\n",
        "  tmpmu=gmm_test.mu\n",
        "  tmpcov=gmm_test.cov\n",
        "  tmppi=gmm_test.pi\n",
        "  \n",
        "home = home.reshape(height*width,channel)\n",
        "#Replace the pixel values of the original image with the mean values\n",
        "for i in range(height*width):\n",
        "  home[i,:]=gmm_test.mu[gmm_test.Z[i]]\n",
        "\n",
        "home=home.reshape(height,width,channel)\n",
        "plt.imshow(home)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4QnyypDpwmW",
        "outputId": "bbb86edc-ea6c-49f6-a57f-6b59a208e353"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "cv2.imwrite('resultimg.jpg',cv2.cvtColor(home, cv2.COLOR_RGB2BGR))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ECSE343 Final Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}